name: Run JMeter Performance Tests

on:
  repository_dispatch:
    types: [run-jmeter-tests]

  workflow_dispatch:
    inputs:
      base_url:
        description: 'URL base de la API a probar'

        required: true
        default: 'http://localhost:8000'
        type: string
      test_plan:
        description: 'Ruta de Test plan a ejecutar'
        required: false
        default: 'test-plans/jmeter/tp-carga-controlada.jmx'
        type: string

permissions:
  contents: write

env:
  PYTHON_VERSION: '3.11'
  JMETER_VERSION: '5.6.3'

jobs:
  run-performance-tests:
    name: Ejecutar Pruebas de Performance con Docker
    runs-on: ubuntu-latest
    
    steps:
      # ============================================================
      # 1. CHECKOUT DEL CÃ“DIGO
      - name: ğŸ“¥ Checkout cÃ³digo
        uses: actions/checkout@v4
      
      # 2. CONFIGURAR DOCKER BUILDX
      - name: ğŸ³ Configurar Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      # 3. CONFIGURAR VARIABLES DE ENTORNO
      - name: âš™ï¸ Configurar variables de entorno
        id: config
        run: |
          # Variables del test (prioridad: workflow_dispatch > repository_dispatch > default)
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            BASE_URL="${{ inputs.base_url }}"
            TEST_PLAN="${{ inputs.test_plan }}"
          elif [ "${{ github.event_name }}" == "repository_dispatch" ]; then
            BASE_URL="${{ github.event.client_payload.base_url || 'http://localhost:8000' }}"
            TEST_PLAN="${{ github.event.client_payload.test_plan || 'test-plans/jmeter/tp-carga-controlada.jmx' }}"
          else
            BASE_URL="http://localhost:8000"
            TEST_PLAN="test-plans/jmeter/tp-carga-controlada.jmx"
          fi
          
          echo "base_url=$BASE_URL" >> $GITHUB_OUTPUT
          echo "test_plan=$TEST_PLAN" >> $GITHUB_OUTPUT
          
          echo "BASE_URL=$BASE_URL" >> $GITHUB_ENV
          echo "TEST_PLAN=$TEST_PLAN" >> $GITHUB_ENV
      
      # 4. MOSTRAR CONFIGURACIÃ“N
      - name: ğŸ“‹ Mostrar configuraciÃ³n del test
        run: |
          echo "================================================"
          echo "  CONFIGURACIÃ“N DE LA PRUEBA"
          echo "================================================"
          echo "Base URL:    ${{ steps.config.outputs.base_url }}"
          echo "Test Plan:   ${{ steps.config.outputs.test_plan }}"
          echo "JMeter:      ${JMETER_VERSION}"
          echo "Python:      ${{ env.PYTHON_VERSION }}"
          echo "Runner:      Docker"
          echo "================================================"
      
      # 5. CACHE DE DOCKER LAYERS
      - name: ğŸ’¾ Cache de Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-
      
      # 6. BUILD DE LA IMAGEN DOCKER
      - name: ğŸ”¨ Build imagen Docker de JMeter
        run: |
          echo "ğŸ”¨ Construyendo imagen Docker..."
          docker build \
            --build-arg JMETER_VERSION=${JMETER_VERSION} \
            --cache-from type=local,src=/tmp/.buildx-cache \
            --cache-to type=local,dest=/tmp/.buildx-cache-new \
            -t jmeter-runner:latest \
            -f Dockerfile \
            .
          
          echo "âœ… Imagen construida exitosamente"
          docker images | grep jmeter-runner
          
          # Mover cache para la prÃ³xima vez
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache || true
      
      # 7. VALIDAR API (si es necesario)
      - name: ğŸ” Validar que la API estÃ© disponible
        if: ${{ !contains(steps.config.outputs.base_url, 'localhost') }}
        run: |
          echo "ğŸ” Verificando disponibilidad de: ${{ steps.config.outputs.base_url }}"
          
          MAX_RETRIES=5
          RETRY_COUNT=0
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if curl -sf "${{ steps.config.outputs.base_url }}/health" > /dev/null 2>&1 || \
               curl -sf "${{ steps.config.outputs.base_url }}" > /dev/null 2>&1; then
              echo "âœ… API disponible"
              exit 0
            fi
            
            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "âš ï¸  Intento $RETRY_COUNT/$MAX_RETRIES - API no disponible, esperando..."
            sleep 10
          done
          
          echo "âŒ API no disponible despuÃ©s de $MAX_RETRIES intentos"
          echo "âš ï¸  Continuando de todas formas..."
      
      # 8. CREAR DIRECTORIOS PARA RESULTADOS
      - name: ğŸ“‚ Crear directorios
        run: |
          mkdir -p results
          mkdir -p docs
          chmod 777 results docs
      
      # 9. EJECUTAR PRUEBAS CON DOCKER
      - name: ğŸš€ Ejecutar pruebas de JMeter en Docker
        run: |
          echo "ğŸ³ Ejecutando contenedor Docker..."
          
          docker run --rm \
            -v $(pwd)/results:/tests/results \
            -v $(pwd)/docs:/tests/docs \
            -v $(pwd)/test-plans:/tests/test-plans:ro \
            -v $(pwd)/data:/tests/data:ro \
            -e BASE_URL="${{ steps.config.outputs.base_url }}" \
            -e TEST_PLAN="${{ steps.config.outputs.test_plan }}" \
            jmeter-runner:latest \
            python3 scripts/execute-tests.py \
              --base-url="${{ steps.config.outputs.base_url }}" \
              --test-plan="${{ steps.config.outputs.test_plan }}"
          
          echo "âœ… Pruebas completadas"
      
      # 10. VERIFICAR RESULTADOS GENERADOS
      - name: ğŸ” Verificar resultados generados
        run: |
          echo "ğŸ“‚ Verificando estructura de archivos generados..."
          
          if [ -d "results" ]; then
            echo "âœ… Directorio results/ existe"
            ls -lah results/
          else
            echo "âŒ No se encontrÃ³ directorio results/"
            exit 1
          fi
          
          if [ -d "docs" ]; then
            echo "âœ… Directorio docs/ existe"
            ls -lah docs/
          else
            echo "âŒ No se encontrÃ³ directorio docs/"
            exit 1
          fi
          
          # Verificar que se generÃ³ el reporte
          if [ -f "docs/index.html" ]; then
            echo "âœ… Reporte HTML generado"
          else
            echo "âŒ No se generÃ³ el reporte HTML"
            exit 1
          fi
      
      # 11. PUBLICAR RESULTADOS
      - name: ğŸ“Š Publicar reporte HTML
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: jmeter-report-${{ github.run_number }}
          path: |
            docs/
            results/run_*/
          retention-days: 30
      
      # 12. DEPLOY A GITHUB PAGES
      - name: ğŸŒ Deploy a GitHub Pages
        if: success()
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs
          publish_branch: gh-pages
          force_orphan: true
          commit_message: 'Deploy JMeter report - Run ${{ github.run_number }}'
      
      # 13. ANALIZAR RESULTADOS
      - name: ğŸ“ˆ Analizar resultados
        if: success()
        run: |
          echo "ğŸ“Š Analizando resultados..."
          
          # Buscar el archivo statistics.json mÃ¡s reciente
          STATS_FILE=$(find results -name "statistics.json" -type f -printf '%T@ %p\n' 2>/dev/null | sort -n | tail -1 | cut -f2- -d" ")
          
          if [ -z "$STATS_FILE" ] || [ ! -f "$STATS_FILE" ]; then
            echo "âš ï¸  No se encontrÃ³ archivo de estadÃ­sticas"
            echo "â„¹ï¸  El test se ejecutÃ³ pero no hay datos para analizar"
            exit 0
          fi
          
          echo "ğŸ“„ Archivo de estadÃ­sticas: $STATS_FILE"
          
          # Extraer mÃ©tricas usando Python
          python3 << EOF
          import json
          import sys
          
          with open("$STATS_FILE", "r") as f:
              stats = json.load(f)
          
          total = stats.get("Total", {})
          error_pct = total.get("errorPct", 0)
          mean_time = total.get("meanResTime", 0)
          p95_time = total.get("pct2ResTime", 0)
          sample_count = total.get("sampleCount", 0)
          
          print(f"\n{'='*60}")
          print(f"  RESULTADOS DE LA PRUEBA")
          print(f"{'='*60}")
          print(f"Total peticiones:   {sample_count:,}")
          print(f"Tasa de error:      {error_pct:.2f}%")
          print(f"Tiempo promedio:    {mean_time:.2f}ms")
          print(f"Percentil 95:       {p95_time:.2f}ms")
          print(f"{'='*60}\n")
          
          # Definir umbrales
          ERROR_THRESHOLD = 5.0  # 5% de errores mÃ¡ximo
          P95_THRESHOLD = 2000   # 2000ms mÃ¡ximo para P95
          
          failed = False
          
          if error_pct > ERROR_THRESHOLD:
              print(f"âŒ FALLO: Tasa de error ({error_pct:.2f}%) supera el umbral ({ERROR_THRESHOLD}%)")
              failed = True
          
          if p95_time > P95_THRESHOLD:
              print(f"âŒ FALLO: P95 ({p95_time:.2f}ms) supera el umbral ({P95_THRESHOLD}ms)")
              failed = True
          
          if not failed:
              print("âœ… Ã‰XITO: Todos los umbrales fueron cumplidos")
          
          # Guardar mÃ©tricas para el siguiente step
          with open("metrics.txt", "w") as f:
              f.write(f"error_pct={error_pct}\n")
              f.write(f"mean_time={mean_time}\n")
              f.write(f"p95_time={p95_time}\n")
              f.write(f"sample_count={sample_count}\n")
              f.write(f"failed={failed}\n")
          
          sys.exit(1 if failed else 0)
          EOF
      
      # 14. GUARDAR MÃ‰TRICAS COMO OUTPUT
      - name: ğŸ’¾ Guardar mÃ©tricas
        if: success() || failure()
        id: metrics
        run: |
          if [ -f "metrics.txt" ]; then
            cat metrics.txt >> $GITHUB_OUTPUT
          fi
      
      # ============================================================
      # 15. COMENTAR EN EL PR (si aplica)
      # ============================================================
      - name: ğŸ’¬ Comentar resultados en PR
        if: github.event_name == 'repository_dispatch' && github.event.client_payload.pr_number
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.PAT_TOKEN || secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            
            let metrics = {
              error_pct: 0,
              mean_time: 0,
              p95_time: 0,
              sample_count: 0,
              failed: false
            };
            
            // Leer mÃ©tricas del archivo
            if (fs.existsSync('metrics.txt')) {
              const content = fs.readFileSync('metrics.txt', 'utf8');
              content.split('\n').forEach(line => {
                const [key, value] = line.split('=');
                if (key && value) {
                  metrics[key] = key === 'failed' ? value === 'True' : parseFloat(value);
                }
              });
            }
            
            const reportUrl = `https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/`;
            const workflowUrl = `${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`;
            
            const statusEmoji = metrics.failed ? 'âŒ' : 'âœ…';
            const statusText = metrics.failed ? 'FALLÃ“' : 'EXITOSO';
            
            const comment = `## ğŸš€ Resultados de Pruebas de Performance ${statusEmoji}
            
            **Estado**: ${statusText}
            
            | MÃ©trica | Valor | Estado |
            |---------|-------|--------|
            | Total peticiones | ${metrics.sample_count.toLocaleString()} | â„¹ï¸ |
            | Tasa de error | ${metrics.error_pct.toFixed(2)}% | ${metrics.error_pct > 5 ? 'âŒ' : 'âœ…'} |
            | Tiempo promedio | ${metrics.mean_time.toFixed(2)}ms | â„¹ï¸ |
            | Percentil 95 | ${metrics.p95_time.toFixed(2)}ms | ${metrics.p95_time > 2000 ? 'âŒ' : 'âœ…'} |
            
            ğŸ“Š [Ver reporte completo](${reportUrl})
            ğŸ”— [Ver ejecuciÃ³n](${workflowUrl})
            
            <details>
            <summary>ConfiguraciÃ³n del test</summary>
            
            - ğŸŒ Base URL: \`${{ github.event.client_payload.base_url }}\`
            - ğŸ‘¥ Threads: ${{ github.event.client_payload.threads }}
            - â±ï¸  DuraciÃ³n: ${{ github.event.client_payload.duration }}s
            - ğŸ“„ Test Plan: \`${{ github.event.client_payload.test_plan }}\`
            - ğŸ³ Runner: Docker
            
            </details>
            `;
            
            // Comentar en el repositorio de origen
            const [owner, repo] = '${{ github.event.client_payload.source_repo }}'.split('/');
            
            await github.rest.issues.createComment({
              owner,
              repo,
              issue_number: ${{ github.event.client_payload.pr_number }},
              body: comment
            });
      
      # 16. LIMPIAR CONTENEDORES E IMÃGENES
      - name: ğŸ§¹ Limpiar Docker
        if: always()
        run: |
          echo "ğŸ§¹ Limpiando recursos de Docker..."
          docker system prune -f
          echo "âœ… Limpieza completada"
      
      # 17. NOTIFICAR RESULTADO
      - name: ğŸ“§ Notificar resultado
        if: always()
        run: |
          if [ "${{ job.status }}" == "success" ]; then
            echo "âœ… Las pruebas de performance completaron exitosamente"
          else
            echo "âŒ Las pruebas de performance fallaron"
            echo "ğŸ“Š Revisa los artefactos para mÃ¡s detalles"
          fi
          echo "ğŸ”— URL del workflow: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"